import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
import org.apache.spark.storage.StorageLevel

object CCFDataFrame {
  def main(args: Array[String]): Unit = {
    // On r√©cup√®re l'URL du master Spark pass√©e en argument, ou bien on travaille en local par d√©faut (pratique pour les tests)
    val masterUrl = if (args.length > 0) args(0) else "local[*]"

    // Initialisation de la session Spark
    val spark = SparkSession.builder()
      .appName("CCF Scala DataFrame")
      .master(masterUrl)
      .config("spark.sql.adaptive.enabled", "false") // D√©sactivation de l'optimisation adaptative pour garder le contr√¥le
      .getOrCreate()

    import spark.implicits._

    // Liste des fichiers CSV √† analyser, chacun repr√©sentant un graphe avec un nombre d'ar√™tes diff√©rent
    val files = Seq(
      ("data/G1_1k.csv", "G1"),
      ("data/G2_5k.csv", "G2"),
      ("data/G3_8k.csv", "G3"),
      ("data/G4_10k.csv", "G4")
    )

    // On va stocker ici les r√©sultats pour chaque graphe
    var results = Seq.empty[(String, Long, Long, Int, Double)]

    // On traite chaque fichier un par un
    files.foreach { case (path, label) =>
      println(s"\nüìé Traitement du graphe $label ($path)")

      // Chargement du fichier CSV avec Spark
      val df = spark.read
        .option("header", "true")       // Les fichiers ont des en-t√™tes
        .option("inferSchema", "true")  // Spark devine automatiquement le type des colonnes
        .csv(path)
        .toDF("source", "target")       // On renomme les colonnes pour plus de clart√©
        .withColumn("source", col("source").cast("int"))  // On convertit les ID en entiers
        .withColumn("target", col("target").cast("int"))
        .na.drop() // On enl√®ve les lignes avec des valeurs manquantes, si jamais

      // Nombre total d'ar√™tes (lignes)
      val nbEdges = df.count()

      // Pour avoir tous les sommets uniques, on fusionne les colonnes source et target, puis on d√©duplique
      val nbNodes = df.select("source").union(df.select("target")).distinct().count()

      // On applique l'algorithme CCF (Connected Components with DataFrame)
      val (components, iters, duration) = ccfDF(df, spark)

      // Quelques infos pour le suivi
      println(s"üîÅ Nombre d'it√©rations effectu√©es : $iters")
      println(f"‚è±Ô∏è Temps d'ex√©cution : $duration%.3f secondes")
      println("-" * 40)

      // On sauvegarde les r√©sultats pour ce graphe
      results = results :+ (label, nbNodes, nbEdges, iters, duration)
    }

    // R√©capitulatif final des performances pour chaque graphe
    val dfResults = results.toDF("Graphe", "Noeuds", "Ar√™tes", "It√©rations", "Temps (s)")
    println("\n‚úÖ R√©sum√© des performances (DataFrame) :")
    dfResults.show(truncate = false)

    println("\nüèÅ Fin de l'ex√©cution du programme CCF avec DataFrame")
    spark.stop()
  }


  // Fonction principale qui ex√©cute l‚Äôalgorithme des composantes connexes avec DataFrame
  def ccfDF(df: DataFrame, spark: SparkSession, maxIters: Int = 50): (DataFrame, Int, Double) = {
    import spark.implicits._

    // √âtape 1 : initialisation - chaque n≈ìud commence dans sa propre composante
    var components = df.select($"source").union(df.select($"target")).distinct()
      .withColumnRenamed("source", "node")
      .withColumn("comp", $"node") // Au d√©but, chaque n≈ìud est sa propre composante
      .persist(StorageLevel.MEMORY_AND_DISK)

    // On construit les voisins en doublant les liens dans les deux sens (non-orient√©)
    val neighbors = df.union(df.select($"target".as("source"), $"source".as("target")))

    var converged = false
    var i = 0
    val startTime = System.nanoTime()

    // Boucle principale de l‚Äôalgorithme
    while (!converged && i < maxIters) {
      // On propage les identifiants de composantes entre voisins
      val joined = neighbors.join(components, $"source" === $"node")

      val propagated = joined.select($"target".as("node"), $"comp")
        .union(joined.select($"source".as("node"), $"comp"))

      // Chaque n≈ìud choisit le plus petit ID de composante re√ßu
      val newComponents = propagated.groupBy("node").agg(min("comp").as("comp"))

      // On v√©rifie si quelque chose a chang√© (si des n≈ìuds ont chang√© de composante)
      val changes = components.join(newComponents, Seq("node"))
        .filter(components("comp") =!= newComponents("comp"))
        .count()

      // On met √† jour les composantes
      val oldComponents = components
      components = newComponents.persist(StorageLevel.MEMORY_AND_DISK)
      oldComponents.unpersist()

      // Si plus aucun changement, on s‚Äôarr√™te
      converged = (changes == 0)
      i += 1
    }

    val durationSec = (System.nanoTime() - startTime) / 1e9

    // Lib√©ration de la m√©moire (important avec Spark)
    components.unpersist()

    (components, i, durationSec)
  }
}

package ccf

import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.rdd.RDD
import org.apache.spark.graphx._
import org.apache.spark.sql.functions._

object CCFGraphX {
  def main(args: Array[String]): Unit = {
    // D√©marrage de la session Spark
    val spark = SparkSession.builder()
      .appName("CCF GraphX")
      .master("local[*]") // Mode local pour l'ex√©cution sur une seule machine
      .getOrCreate()

    import spark.implicits._

    // Liste des fichiers √† analyser (chaque fichier repr√©sente un graphe)
    val files = Seq(
      ("data/G1_1k.csv", "G1"),
      ("data/G2_5k.csv", "G2"),
      ("data/G3_8k.csv", "G3"),
      ("data/G4_10k.csv", "G4")
    )

    // Contiendra les r√©sultats pour chaque graphe
    var results = Seq.empty[(String, Long, Long, Double)]

    // On traite les fichiers un par un
    files.foreach { case (path, label) =>
      println(s"\nüìé Traitement du graphe $label ($path)")

      // Chargement du fichier CSV avec Spark
      val df = spark.read
        .option("header", "true")       // Les fichiers ont des en-t√™tes
        .option("inferSchema", "true")  // On laisse Spark deviner les types de colonnes
        .csv(path)
        .toDF("source", "target")
        .withColumn("source", col("source").cast("long"))
        .withColumn("target", col("target").cast("long"))
        .na.drop() // Suppression des lignes incompl√®tes s‚Äôil y en a

      val nbEdges = df.count() // Nombre d‚Äôar√™tes
      val nbNodes = df.select("source").union(df.select("target")).distinct().count() // Sommets uniques

      // Ex√©cution de l'algorithme CCF avec GraphX
      val (components, duration) = ccfGraphX(df, spark)

      println(f" Temps d'ex√©cution : $duration%.3f sec")
      println("-" * 40)

      // Ajout des r√©sultats √† la liste
      results = results :+ (label, nbNodes, nbEdges, duration)
    }

    // Affichage du r√©sum√© final
    val dfResults = results.toDF("Graphe", "Noeuds", "Ar√™tes", "Temps (s)")
    println("\n R√©sum√© des performances (GraphX) :")
    dfResults.show(truncate = false)

    println("\n Fin du programme CCF GraphX")
    spark.stop()
  }

  // Fonction qui ex√©cute le CCF en utilisant l‚ÄôAPI GraphX
  def ccfGraphX(df: DataFrame, spark: SparkSession): (DataFrame, Double) = {
    import spark.implicits._

    val startTime = System.nanoTime()

    // Transformation du DataFrame en RDD d‚Äôar√™tes compatibles avec GraphX
    val edgesRDD: RDD[Edge[Long]] = df.rdd.map(row => {
      val src = row.getAs[Long]("source")
      val dst = row.getAs[Long]("target")
      Edge(src, dst)
    })

    // Construction du graphe √† partir des ar√™tes
    val graph = Graph.fromEdges(edgesRDD, defaultValue = 1L)

    // Utilisation de l‚Äôalgorithme Connected Components de GraphX
    val cc = graph.connectedComponents()

    // R√©sultat sous forme de DataFrame (n≈ìud, composante)
    val componentsDF = cc.vertices.toDF("node", "component")

    // On d√©clenche l'ex√©cution pour mesurer le temps r√©el (sinon c‚Äôest paresseux)
    componentsDF.count()

    val duration = (System.nanoTime() - startTime) / 1e9

    (componentsDF, duration)
  }
}

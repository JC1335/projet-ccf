{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9XJIg0qojvgCUW6GdtoEJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JC1335/projet-ccf/blob/master/Final_Projet_Graph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 **Programmation en RDDs (PySpark) - Python**"
      ],
      "metadata": {
        "id": "uNiPDXMC376N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5cAqROqr30gL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39e27be1-8f6e-4dee-bdb3-1169fd4ad488"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìé Traitement de G1 (data/G1_1k.csv)...\n",
            "‚ùå Erreur avec G1 : [PATH_NOT_FOUND] Path does not exist: file:/content/data/G1_1k.csv.\n",
            "----------------------------------------\n",
            "üìé Traitement de G2 (data/G2_5k.csv)...\n",
            "‚ùå Erreur avec G2 : [PATH_NOT_FOUND] Path does not exist: file:/content/data/G2_5k.csv.\n",
            "----------------------------------------\n",
            "üìé Traitement de G3 (data/G3_8k.csv)...\n",
            "‚ùå Erreur avec G3 : [PATH_NOT_FOUND] Path does not exist: file:/content/data/G3_8k.csv.\n",
            "----------------------------------------\n",
            "üìé Traitement de G4 (data/G4_10k.csv)...\n",
            "‚ùå Erreur avec G4 : [PATH_NOT_FOUND] Path does not exist: file:/content/data/G4_10k.csv.\n",
            "----------------------------------------\n",
            "‚úÖ R√©sum√© des performances (RDD) :\n",
            "Empty DataFrame\n",
            "Columns: [Graphe, N≈ìuds, Ar√™tes, It√©rations, Temps (s)]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Initialisation de SparkSession et du SparkContext\n",
        "spark = SparkSession.builder.appName(\"CCF Correct RDD\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Accumulateur pour suivre les nouveaux couples (comme d√©crit dans le papier)\n",
        "new_pairs_counter = sc.accumulator(0)\n",
        "\n",
        "def ccf_correct_implementation(edges_rdd, max_iters=20):\n",
        "    \"\"\"\n",
        "    Impl√©mentation correcte de l'algorithme CCF (Connected Component Finder)\n",
        "    en utilisant les RDD PySpark, suivant la logique du papier acad√©mique.\n",
        "    \"\"\"\n",
        "    global new_pairs_counter\n",
        "\n",
        "    # √âtape 1 : Initialiser chaque n≈ìud avec sa propre √©tiquette de composante\n",
        "    nodes = edges_rdd.flatMap(lambda edge: [edge[0], edge[1]]).distinct()\n",
        "    node_component_rdd = nodes.map(lambda node: (node, node))  # (n≈ìud, id_de_composante)\n",
        "\n",
        "    # √âtape 2 : Cr√©er la liste d‚Äôadjacence (bidirectionnelle)\n",
        "    neighbors_rdd = edges_rdd.flatMap(lambda x: [(x[0], x[1]), (x[1], x[0])])\n",
        "\n",
        "    iteration = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Boucle principale : on propage les composantes jusqu‚Äô√† convergence ou it√©rations max\n",
        "    while iteration < max_iters:\n",
        "        iteration += 1\n",
        "        print(f\"üîÅ D√©marrage de l'it√©ration {iteration}...\")\n",
        "\n",
        "        # R√©initialisation de l'accumulateur pour la nouvelle it√©ration\n",
        "        new_pairs_counter.value = 0\n",
        "\n",
        "        # 1Ô∏è Joindre chaque n≈ìud avec la composante de ses voisins\n",
        "        joined_rdd = node_component_rdd.join(neighbors_rdd).map(\n",
        "            lambda x: (x[1][1], x[1][0])  # (voisin, composante du n≈ìud)\n",
        "        )\n",
        "\n",
        "        # 2Ô∏è Grouper toutes les composantes possibles pour chaque n≈ìud\n",
        "        input_for_reducer = joined_rdd.union(node_component_rdd).groupByKey()\n",
        "\n",
        "        # 3Ô∏è Phase de r√©duction (comme d√©crite dans la Figure 2 du papier)\n",
        "        def ccf_iterate_reducer(key_values):\n",
        "            key, values_iter = key_values\n",
        "            values = list(values_iter)\n",
        "            min_val = min(values)\n",
        "\n",
        "            if min_val < key:\n",
        "                # Ce n≈ìud peut adopter une composante plus petite : on propage\n",
        "                yield (key, min_val)\n",
        "                for val in values:\n",
        "                    if val != min_val:\n",
        "                        new_pairs_counter.add(1)\n",
        "                        yield (val, min_val)\n",
        "            else:\n",
        "                # Sinon, il garde sa propre √©tiquette\n",
        "                yield (key, key)\n",
        "\n",
        "        # Application du reducer et d√©doublonnage\n",
        "        ccf_iterate_output = input_for_reducer.flatMap(ccf_iterate_reducer)\n",
        "        dedup_output = ccf_iterate_output.distinct()\n",
        "\n",
        "        # Mise √† jour de la RDD des composantes\n",
        "        node_component_rdd = dedup_output\n",
        "\n",
        "        # Action obligatoire pour forcer l‚Äô√©valuation du compteur (Spark est paresseux)\n",
        "        node_component_rdd.count()\n",
        "\n",
        "        # Si aucune propagation n‚Äôa eu lieu, on consid√®re qu‚Äôon a converg√©\n",
        "        if new_pairs_counter.value == 0:\n",
        "            print(f\"‚úÖ Convergence atteinte en {iteration} it√©rations.\")\n",
        "            break\n",
        "\n",
        "    exec_time = time.time() - start_time\n",
        "    return node_component_rdd, iteration, exec_time\n",
        "\n",
        "# Chargement des fichiers et ex√©cution du CCF pour chacun ---\n",
        "schema = StructType([\n",
        "    StructField(\"source\", IntegerType(), True),\n",
        "    StructField(\"target\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Liste des fichiers √† analyser\n",
        "files = [\n",
        "    (\"G1_1k.csv\", \"G1\"),\n",
        "    (\"G2_5k.csv\", \"G2\"),\n",
        "    (\"G3_8k.csv\", \"G3\"),\n",
        "    (\"G4_10k.csv\", \"G4\")\n",
        "]\n",
        "\n",
        "# Liste pour stocker les r√©sultats\n",
        "results = []\n",
        "\n",
        "for filename, label in files:\n",
        "    filepath = f\"data/{filename}\"  # Assure-toi que le fichier est bien dans le dossier \"data\"\n",
        "    print(f\"üìé Traitement de {label} ({filepath})...\")\n",
        "\n",
        "    try:\n",
        "        # Chargement du fichier CSV en DataFrame structur√©\n",
        "        df = spark.read.csv(filepath, header=True, schema=schema)\n",
        "\n",
        "        # Conversion en RDD de paires (source, target)\n",
        "        edges_rdd = df.rdd.map(lambda row: (row['source'], row['target']))\n",
        "\n",
        "        # Lancement de l'algorithme CCF\n",
        "        components, num_iters, exec_time = ccf_correct_implementation(edges_rdd, max_iters=20)\n",
        "\n",
        "        # üìà Statistiques du graphe\n",
        "        nb_nodes = edges_rdd.flatMap(lambda edge: [edge[0], edge[1]]).distinct().count()\n",
        "        nb_edges = edges_rdd.count()\n",
        "\n",
        "        print(f\"üìä Donn√©es du graphe: {nb_nodes} n≈ìuds, {nb_edges} ar√™tes\")\n",
        "        print(f\"üîÅ It√©rations : {num_iters}\")\n",
        "        print(f\"‚è±Ô∏è Temps : {round(exec_time, 3)} secondes\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Ajout aux r√©sultats\n",
        "        results.append((label, nb_nodes, nb_edges, num_iters, round(exec_time, 3)))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur avec {label} : {e}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "# Affichage final dans un tableau Pandas\n",
        "result_rdd_df = pd.DataFrame(\n",
        "    results,\n",
        "    columns=[\"Graphe\", \"N≈ìuds\", \"Ar√™tes\", \"It√©rations\", \"Temps (s)\"]\n",
        ")\n",
        "\n",
        "print(\"‚úÖ R√©sum√© des performances (RDD) :\")\n",
        "print(result_rdd_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2 \tImpl√©mentation CCF avec DataFrames _ Python**"
      ],
      "metadata": {
        "id": "nwAnmF7B4PFM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt0YUWXU6K9Y",
        "outputId": "9e65b3a8-dc10-4c17-a2f1-74c70794e793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìé Traitement de G1 (data/G1_1k.csv)...\n",
            "‚ùå Erreur avec G1 : [PATH_NOT_FOUND] Path does not exist: file:/content/data/G1_1k.csv.\n",
            "----------------------------------------\n",
            "üìé Traitement de G2 (data/G2_5k.csv)...\n",
            "‚ùå Erreur avec G2 : [PATH_NOT_FOUND] Path does not exist: file:/content/data/G2_5k.csv.\n",
            "----------------------------------------\n",
            "üìé Traitement de G3 (data/G3_8k.csv)...\n",
            "‚ùå Erreur avec G3 : [PATH_NOT_FOUND] Path does not exist: file:/content/data/G3_8k.csv.\n",
            "----------------------------------------\n",
            "üìé Traitement de G4 (data/G4_10k.csv)...\n",
            "‚ùå Erreur avec G4 : [PATH_NOT_FOUND] Path does not exist: file:/content/data/G4_10k.csv.\n",
            "----------------------------------------\n",
            "‚úÖ R√©sum√© des performances (DataFrame) :\n",
            "Empty DataFrame\n",
            "Columns: [Graphe, N≈ìuds, Ar√™tes, It√©rations, Temps (s)]\n",
            "Index: []\n"
          ]
        }
      ]
    }
  ]
}
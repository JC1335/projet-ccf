{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzcyIWINEEcqFSF5JvT4tE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JC1335/projet-ccf/blob/master/Final_Projet_Graph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 **Programmation en RDDs (PySpark) - Python**"
      ],
      "metadata": {
        "id": "uNiPDXMC376N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5cAqROqr30gL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a95327d-dfca-404a-fe4e-c4ee18e77ab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📎 Traitement de G1 (data/G1_1k.csv)...\n",
            " Démarrage de l'itération 1...\n",
            " Démarrage de l'itération 2...\n",
            " Démarrage de l'itération 3...\n",
            " Démarrage de l'itération 4...\n",
            " Démarrage de l'itération 5...\n",
            "✅ Convergence atteinte en 5 itérations.\n",
            "📊 Données du graphe: 996 nœuds, 3000 arêtes\n",
            "🔁 Itérations : 5\n",
            "⏱️ Temps : 82.175 secondes\n",
            "----------------------------------------\n",
            "📎 Traitement de G2 (data/G2_5k.csv)...\n",
            " Démarrage de l'itération 1...\n",
            " Démarrage de l'itération 2...\n",
            " Démarrage de l'itération 3...\n",
            " Démarrage de l'itération 4...\n",
            " Démarrage de l'itération 5...\n",
            " Démarrage de l'itération 6...\n",
            "✅ Convergence atteinte en 6 itérations.\n",
            "📊 Données du graphe: 4983 nœuds, 15000 arêtes\n",
            "🔁 Itérations : 6\n",
            "⏱️ Temps : 150.759 secondes\n",
            "----------------------------------------\n",
            "📎 Traitement de G3 (data/G3_8k.csv)...\n",
            " Démarrage de l'itération 1...\n",
            " Démarrage de l'itération 2...\n",
            " Démarrage de l'itération 3...\n",
            " Démarrage de l'itération 4...\n",
            " Démarrage de l'itération 5...\n",
            " Démarrage de l'itération 6...\n",
            "✅ Convergence atteinte en 6 itérations.\n",
            "📊 Données du graphe: 7971 nœuds, 24000 arêtes\n",
            "🔁 Itérations : 6\n",
            "⏱️ Temps : 148.971 secondes\n",
            "----------------------------------------\n",
            "📎 Traitement de G4 (data/G4_10k.csv)...\n",
            " Démarrage de l'itération 1...\n",
            " Démarrage de l'itération 2...\n",
            " Démarrage de l'itération 3...\n",
            " Démarrage de l'itération 4...\n",
            " Démarrage de l'itération 5...\n",
            " Démarrage de l'itération 6...\n",
            "✅ Convergence atteinte en 6 itérations.\n",
            "📊 Données du graphe: 9979 nœuds, 30000 arêtes\n",
            "🔁 Itérations : 6\n",
            "⏱️ Temps : 150.38 secondes\n",
            "----------------------------------------\n",
            "✅ Résumé des performances (RDD) :\n",
            "  Graphe  Nœuds  Arêtes  Itérations  Temps (s)\n",
            "0     G1    996    3000           5     82.175\n",
            "1     G2   4983   15000           6    150.759\n",
            "2     G3   7971   24000           6    148.971\n",
            "3     G4   9979   30000           6    150.380\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Initialisation de SparkSession et du SparkContext\n",
        "spark = SparkSession.builder.appName(\"CCF Correct RDD\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Accumulateur pour suivre les nouveaux couples\n",
        "new_pairs_counter = sc.accumulator(0)\n",
        "\n",
        "def ccf_correct_implementation(edges_rdd, max_iters=20):\n",
        "    \"\"\"\n",
        "    Implémentation correcte de l'algorithme CCF (Connected Component Finder)\n",
        "    en utilisant les RDD PySpark, suivant la logique du papier académique.\n",
        "    \"\"\"\n",
        "    global new_pairs_counter\n",
        "\n",
        "    # Étape 1 : Initialiser chaque nœud avec sa propre étiquette de composante\n",
        "    nodes = edges_rdd.flatMap(lambda edge: [edge[0], edge[1]]).distinct()\n",
        "    node_component_rdd = nodes.map(lambda node: (node, node))  # (nœud, id_de_composante)\n",
        "\n",
        "    # Étape 2 : Créer la liste d’adjacence (bidirectionnelle)\n",
        "    neighbors_rdd = edges_rdd.flatMap(lambda x: [(x[0], x[1]), (x[1], x[0])])\n",
        "\n",
        "    iteration = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Boucle principale : on propage les composantes jusqu’à convergence ou itérations max\n",
        "    while iteration < max_iters:\n",
        "        iteration += 1\n",
        "        print(f\" Démarrage de l'itération {iteration}...\")\n",
        "\n",
        "        # Réinitialisation de l'accumulateur pour la nouvelle itération\n",
        "        new_pairs_counter.value = 0\n",
        "\n",
        "        # 1️ Joindre chaque nœud avec la composante de ses voisins\n",
        "        joined_rdd = node_component_rdd.join(neighbors_rdd).map(\n",
        "            lambda x: (x[1][1], x[1][0])  # (voisin, composante du nœud)\n",
        "        )\n",
        "\n",
        "        # 2️ Grouper toutes les composantes possibles pour chaque nœud\n",
        "        input_for_reducer = joined_rdd.union(node_component_rdd).groupByKey()\n",
        "\n",
        "        # 3️ Phase de réduction (comme décrite dans la Figure 2 du papier)\n",
        "        def ccf_iterate_reducer(key_values):\n",
        "            key, values_iter = key_values\n",
        "            values = list(values_iter)\n",
        "            min_val = min(values)\n",
        "\n",
        "            if min_val < key:\n",
        "                # Ce nœud peut adopter une composante plus petite : on propage\n",
        "                yield (key, min_val)\n",
        "                for val in values:\n",
        "                    if val != min_val:\n",
        "                        new_pairs_counter.add(1)\n",
        "                        yield (val, min_val)\n",
        "            else:\n",
        "                # Sinon, il garde sa propre étiquette\n",
        "                yield (key, key)\n",
        "\n",
        "        # Application du reducer et dédoublonnage\n",
        "        ccf_iterate_output = input_for_reducer.flatMap(ccf_iterate_reducer)\n",
        "        dedup_output = ccf_iterate_output.distinct()\n",
        "\n",
        "        # Mise à jour de la RDD des composantes\n",
        "        node_component_rdd = dedup_output\n",
        "\n",
        "        # Action obligatoire pour forcer l’évaluation du compteur (Spark est paresseux)\n",
        "        node_component_rdd.count()\n",
        "\n",
        "        # Si aucune propagation n’a eu lieu, on considère qu’on a convergé\n",
        "        if new_pairs_counter.value == 0:\n",
        "            print(f\"✅ Convergence atteinte en {iteration} itérations.\")\n",
        "            break\n",
        "\n",
        "    exec_time = time.time() - start_time\n",
        "    return node_component_rdd, iteration, exec_time\n",
        "\n",
        "# Chargement des fichiers et exécution du CCF pour chacun ---\n",
        "schema = StructType([\n",
        "    StructField(\"source\", IntegerType(), True),\n",
        "    StructField(\"target\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Liste des fichiers à analyser\n",
        "files = [\n",
        "    (\"G1_1k.csv\", \"G1\"),\n",
        "    (\"G2_5k.csv\", \"G2\"),\n",
        "    (\"G3_8k.csv\", \"G3\"),\n",
        "    (\"G4_10k.csv\", \"G4\")\n",
        "]\n",
        "\n",
        "# Liste pour stocker les résultats\n",
        "results = []\n",
        "\n",
        "for filename, label in files:\n",
        "    filepath = f\"data/{filename}\"  # Etre sûr que le fichier est bien dans le dossier \"data\"\n",
        "    print(f\"📎 Traitement de {label} ({filepath})...\")\n",
        "\n",
        "    try:\n",
        "        # Chargement du fichier CSV en DataFrame structuré\n",
        "        df = spark.read.csv(filepath, header=True, schema=schema)\n",
        "\n",
        "        # Conversion en RDD de paires (source, target)\n",
        "        edges_rdd = df.rdd.map(lambda row: (row['source'], row['target']))\n",
        "\n",
        "        # Lancement de l'algorithme CCF\n",
        "        components, num_iters, exec_time = ccf_correct_implementation(edges_rdd, max_iters=20)\n",
        "\n",
        "        # 📈 Statistiques du graphe\n",
        "        nb_nodes = edges_rdd.flatMap(lambda edge: [edge[0], edge[1]]).distinct().count()\n",
        "        nb_edges = edges_rdd.count()\n",
        "\n",
        "        print(f\"📊 Données du graphe: {nb_nodes} nœuds, {nb_edges} arêtes\")\n",
        "        print(f\"🔁 Itérations : {num_iters}\")\n",
        "        print(f\"⏱️ Temps : {round(exec_time, 3)} secondes\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Ajout aux résultats\n",
        "        results.append((label, nb_nodes, nb_edges, num_iters, round(exec_time, 3)))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erreur avec {label} : {e}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "# Affichage final dans un tableau Pandas\n",
        "result_rdd_df = pd.DataFrame(\n",
        "    results,\n",
        "    columns=[\"Graphe\", \"Nœuds\", \"Arêtes\", \"Itérations\", \"Temps (s)\"]\n",
        ")\n",
        "\n",
        "print(\"✅ Résumé des performances (RDD) :\")\n",
        "print(result_rdd_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2 \tImplémentation CCF avec DataFrames _ Python**"
      ],
      "metadata": {
        "id": "nwAnmF7B4PFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, least\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType\n",
        "\n",
        "# Création de la session Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CCF DataFrame Correct\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "def ccf_dataframe_implementation(edges_df, max_iters=20):\n",
        "    \"\"\"\n",
        "    Implémentation de l’algorithme Connected Component Finder (CCF)\n",
        "    en utilisant l'API DataFrame de PySpark.\n",
        "\n",
        "    Objectif : identifier les composantes connexes d'un graphe non orienté\n",
        "    à partir de ses arêtes (edges_df), en propageant les identifiants\n",
        "    de composantes les plus petits jusqu'à convergence.\n",
        "    \"\"\"\n",
        "\n",
        "   # Étape 1 : Initialisation — chaque nœud reçoit une étiquette égale à son propre ID\n",
        "    nodes = edges_df.select(\"source\").union(edges_df.select(\"target\")) \\\n",
        "        .distinct() \\\n",
        "        .withColumnRenamed(\"source\", \"node\")\n",
        "\n",
        "    labels = nodes.withColumn(\"component_id\", col(\"node\"))\n",
        "\n",
        "    iteration = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    # On transforme le graphe en liste d'adjacence non orientée (chaque lien est doublé)\n",
        "    adj_list = edges_df.select(\"source\", \"target\").union(\n",
        "        edges_df.select(col(\"target\").alias(\"source\"), col(\"source\").alias(\"target\"))\n",
        "    )\n",
        "\n",
        "    # Boucle principale : on propage les plus petits IDs de composantes\n",
        "    while iteration < max_iters:\n",
        "        iteration += 1\n",
        "        print(f\"🔁 Démarrage de l'itération {iteration}...\")\n",
        "\n",
        "        # On renomme la colonne pour éviter les conflits lors des jointures\n",
        "        labels_renamed = labels.withColumnRenamed(\"component_id\", \"current_component_id\")\n",
        "\n",
        "        # 1️ Chaque nœud va consulter les étiquettes de ses voisins\n",
        "        new_labels = adj_list.join(labels_renamed, adj_list.target == labels_renamed.node) \\\n",
        "            .select(\n",
        "                adj_list.source.alias(\"node\"),\n",
        "                labels_renamed.current_component_id.alias(\"neighbor_component_id\")\n",
        "            ) \\\n",
        "            .groupBy(\"node\") \\\n",
        "            .agg({\"neighbor_component_id\": \"min\"}) \\\n",
        "            .withColumnRenamed(\"min(neighbor_component_id)\", \"propagated_id\")\n",
        "\n",
        "        # 2️ Jointure avec les étiquettes actuelles pour comparer les valeurs\n",
        "        current_and_new_labels = labels_renamed.join(new_labels, \"node\", \"left_outer\")\n",
        "\n",
        "        # 3️ Mise à jour : on garde l’ID de composante le plus petit entre l’actuel et le propagé\n",
        "        updated_labels = current_and_new_labels \\\n",
        "            .withColumn(\n",
        "                \"new_label\",\n",
        "                least(col(\"current_component_id\"), col(\"propagated_id\"))\n",
        "            ) \\\n",
        "            .select(col(\"node\"), col(\"new_label\").alias(\"component_id\"))\n",
        "\n",
        "        # 4️ Vérification de convergence : on regarde s’il y a encore des changements\n",
        "        changes = updated_labels \\\n",
        "            .join(labels.withColumnRenamed(\"component_id\", \"old_component_id\"), \"node\") \\\n",
        "            .filter(col(\"component_id\") != col(\"old_component_id\")) \\\n",
        "            .count()\n",
        "\n",
        "        # 🔁 On prépare les labels pour la prochaine itération\n",
        "        labels = updated_labels\n",
        "\n",
        "        # Si aucun changement n’a eu lieu, c’est que l’algorithme a convergé\n",
        "        if changes == 0:\n",
        "            print(f\"✅ Convergence atteinte en {iteration} itérations.\")\n",
        "            break\n",
        "\n",
        "    exec_time = time.time() - start_time\n",
        "    return labels, iteration, exec_time\n",
        "\n",
        "\n",
        "# Définition du schéma des fichiers CSV\n",
        "schema = StructType([\n",
        "    StructField(\"source\", IntegerType(), True),\n",
        "    StructField(\"target\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Liste des graphes à traiter\n",
        "files = [\n",
        "    (\"G1_1k.csv\", \"G1\"),\n",
        "    (\"G2_5k.csv\", \"G2\"),\n",
        "    (\"G3_8k.csv\", \"G3\"),\n",
        "    (\"G4_10k.csv\", \"G4\")\n",
        "]\n",
        "\n",
        "# Liste pour stocker les résultats de chaque traitement\n",
        "results = []\n",
        "\n",
        "# Traitement de chaque fichier un par un\n",
        "for filename, label in files:\n",
        "    filepath = f\"data/{filename}\"  # Les fichiers doivent être placés dans un dossier 'data'\n",
        "    print(f\"📎 Traitement de {label} ({filepath})...\")\n",
        "\n",
        "    try:\n",
        "        # Chargement du fichier CSV sous forme de DataFrame\n",
        "        edges_df = spark.read.csv(filepath, header=True, schema=schema)\n",
        "\n",
        "        # Exécution de l'algorithme CCF\n",
        "        components_df, num_iters, exec_time = ccf_dataframe_implementation(edges_df, max_iters=20)\n",
        "\n",
        "        # Calcul du nombre de nœuds et d’arêtes dans le graphe\n",
        "        nb_nodes = edges_df.select(\"source\").union(edges_df.select(\"target\")).distinct().count()\n",
        "        nb_edges = edges_df.count()\n",
        "\n",
        "        print(f\"📊 Données du graphe : {nb_nodes} nœuds, {nb_edges} arêtes\")\n",
        "        print(f\"🔁 Itérations : {num_iters}\")\n",
        "        print(f\"⏱️ Temps d'exécution : {round(exec_time, 3)} secondes\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Stockage des résultats\n",
        "        results.append((label, nb_nodes, nb_edges, num_iters, round(exec_time, 3)))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erreur lors du traitement de {label} : {e}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "# Affichage final des performances sous forme de tableau Pandas\n",
        "result_df = pd.DataFrame(\n",
        "    results,\n",
        "    columns=[\"Graphe\", \"Nœuds\", \"Arêtes\", \"Itérations\", \"Temps (s)\"]\n",
        ")\n",
        "\n",
        "print(\"✅ Résumé des performances (DataFrame) :\")\n",
        "print(result_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt0YUWXU6K9Y",
        "outputId": "8beceb0b-074f-4731-fb53-487b30171d99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📎 Traitement de G1 (data/G1_1k.csv)...\n",
            "🔁 Démarrage de l'itération 1...\n",
            "🔁 Démarrage de l'itération 2...\n",
            "🔁 Démarrage de l'itération 3...\n",
            "🔁 Démarrage de l'itération 4...\n",
            "🔁 Démarrage de l'itération 5...\n",
            "🔁 Démarrage de l'itération 6...\n",
            "🔁 Démarrage de l'itération 7...\n",
            "🔁 Démarrage de l'itération 8...\n",
            "✅ Convergence atteinte en 8 itérations.\n",
            "📊 Données du graphe : 996 nœuds, 3000 arêtes\n",
            "🔁 Itérations : 8\n",
            "⏱️ Temps d'exécution : 140.183 secondes\n",
            "----------------------------------------\n",
            "📎 Traitement de G2 (data/G2_5k.csv)...\n",
            "🔁 Démarrage de l'itération 1...\n",
            "🔁 Démarrage de l'itération 2...\n"
          ]
        }
      ]
    }
  ]
}
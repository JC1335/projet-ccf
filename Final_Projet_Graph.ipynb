{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzcyIWINEEcqFSF5JvT4tE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JC1335/projet-ccf/blob/master/Final_Projet_Graph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 **Programmation en RDDs (PySpark) - Python**"
      ],
      "metadata": {
        "id": "uNiPDXMC376N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5cAqROqr30gL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a95327d-dfca-404a-fe4e-c4ee18e77ab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“ Traitement de G1 (data/G1_1k.csv)...\n",
            " DÃ©marrage de l'itÃ©ration 1...\n",
            " DÃ©marrage de l'itÃ©ration 2...\n",
            " DÃ©marrage de l'itÃ©ration 3...\n",
            " DÃ©marrage de l'itÃ©ration 4...\n",
            " DÃ©marrage de l'itÃ©ration 5...\n",
            "âœ… Convergence atteinte en 5 itÃ©rations.\n",
            "ğŸ“Š DonnÃ©es du graphe: 996 nÅ“uds, 3000 arÃªtes\n",
            "ğŸ” ItÃ©rations : 5\n",
            "â±ï¸ Temps : 82.175 secondes\n",
            "----------------------------------------\n",
            "ğŸ“ Traitement de G2 (data/G2_5k.csv)...\n",
            " DÃ©marrage de l'itÃ©ration 1...\n",
            " DÃ©marrage de l'itÃ©ration 2...\n",
            " DÃ©marrage de l'itÃ©ration 3...\n",
            " DÃ©marrage de l'itÃ©ration 4...\n",
            " DÃ©marrage de l'itÃ©ration 5...\n",
            " DÃ©marrage de l'itÃ©ration 6...\n",
            "âœ… Convergence atteinte en 6 itÃ©rations.\n",
            "ğŸ“Š DonnÃ©es du graphe: 4983 nÅ“uds, 15000 arÃªtes\n",
            "ğŸ” ItÃ©rations : 6\n",
            "â±ï¸ Temps : 150.759 secondes\n",
            "----------------------------------------\n",
            "ğŸ“ Traitement de G3 (data/G3_8k.csv)...\n",
            " DÃ©marrage de l'itÃ©ration 1...\n",
            " DÃ©marrage de l'itÃ©ration 2...\n",
            " DÃ©marrage de l'itÃ©ration 3...\n",
            " DÃ©marrage de l'itÃ©ration 4...\n",
            " DÃ©marrage de l'itÃ©ration 5...\n",
            " DÃ©marrage de l'itÃ©ration 6...\n",
            "âœ… Convergence atteinte en 6 itÃ©rations.\n",
            "ğŸ“Š DonnÃ©es du graphe: 7971 nÅ“uds, 24000 arÃªtes\n",
            "ğŸ” ItÃ©rations : 6\n",
            "â±ï¸ Temps : 148.971 secondes\n",
            "----------------------------------------\n",
            "ğŸ“ Traitement de G4 (data/G4_10k.csv)...\n",
            " DÃ©marrage de l'itÃ©ration 1...\n",
            " DÃ©marrage de l'itÃ©ration 2...\n",
            " DÃ©marrage de l'itÃ©ration 3...\n",
            " DÃ©marrage de l'itÃ©ration 4...\n",
            " DÃ©marrage de l'itÃ©ration 5...\n",
            " DÃ©marrage de l'itÃ©ration 6...\n",
            "âœ… Convergence atteinte en 6 itÃ©rations.\n",
            "ğŸ“Š DonnÃ©es du graphe: 9979 nÅ“uds, 30000 arÃªtes\n",
            "ğŸ” ItÃ©rations : 6\n",
            "â±ï¸ Temps : 150.38 secondes\n",
            "----------------------------------------\n",
            "âœ… RÃ©sumÃ© des performances (RDD) :\n",
            "  Graphe  NÅ“uds  ArÃªtes  ItÃ©rations  Temps (s)\n",
            "0     G1    996    3000           5     82.175\n",
            "1     G2   4983   15000           6    150.759\n",
            "2     G3   7971   24000           6    148.971\n",
            "3     G4   9979   30000           6    150.380\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Initialisation de SparkSession et du SparkContext\n",
        "spark = SparkSession.builder.appName(\"CCF Correct RDD\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Accumulateur pour suivre les nouveaux couples\n",
        "new_pairs_counter = sc.accumulator(0)\n",
        "\n",
        "def ccf_correct_implementation(edges_rdd, max_iters=20):\n",
        "    \"\"\"\n",
        "    ImplÃ©mentation correcte de l'algorithme CCF (Connected Component Finder)\n",
        "    en utilisant les RDD PySpark, suivant la logique du papier acadÃ©mique.\n",
        "    \"\"\"\n",
        "    global new_pairs_counter\n",
        "\n",
        "    # Ã‰tape 1 : Initialiser chaque nÅ“ud avec sa propre Ã©tiquette de composante\n",
        "    nodes = edges_rdd.flatMap(lambda edge: [edge[0], edge[1]]).distinct()\n",
        "    node_component_rdd = nodes.map(lambda node: (node, node))  # (nÅ“ud, id_de_composante)\n",
        "\n",
        "    # Ã‰tape 2 : CrÃ©er la liste dâ€™adjacence (bidirectionnelle)\n",
        "    neighbors_rdd = edges_rdd.flatMap(lambda x: [(x[0], x[1]), (x[1], x[0])])\n",
        "\n",
        "    iteration = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Boucle principale : on propage les composantes jusquâ€™Ã  convergence ou itÃ©rations max\n",
        "    while iteration < max_iters:\n",
        "        iteration += 1\n",
        "        print(f\" DÃ©marrage de l'itÃ©ration {iteration}...\")\n",
        "\n",
        "        # RÃ©initialisation de l'accumulateur pour la nouvelle itÃ©ration\n",
        "        new_pairs_counter.value = 0\n",
        "\n",
        "        # 1ï¸ Joindre chaque nÅ“ud avec la composante de ses voisins\n",
        "        joined_rdd = node_component_rdd.join(neighbors_rdd).map(\n",
        "            lambda x: (x[1][1], x[1][0])  # (voisin, composante du nÅ“ud)\n",
        "        )\n",
        "\n",
        "        # 2ï¸ Grouper toutes les composantes possibles pour chaque nÅ“ud\n",
        "        input_for_reducer = joined_rdd.union(node_component_rdd).groupByKey()\n",
        "\n",
        "        # 3ï¸ Phase de rÃ©duction (comme dÃ©crite dans la Figure 2 du papier)\n",
        "        def ccf_iterate_reducer(key_values):\n",
        "            key, values_iter = key_values\n",
        "            values = list(values_iter)\n",
        "            min_val = min(values)\n",
        "\n",
        "            if min_val < key:\n",
        "                # Ce nÅ“ud peut adopter une composante plus petite : on propage\n",
        "                yield (key, min_val)\n",
        "                for val in values:\n",
        "                    if val != min_val:\n",
        "                        new_pairs_counter.add(1)\n",
        "                        yield (val, min_val)\n",
        "            else:\n",
        "                # Sinon, il garde sa propre Ã©tiquette\n",
        "                yield (key, key)\n",
        "\n",
        "        # Application du reducer et dÃ©doublonnage\n",
        "        ccf_iterate_output = input_for_reducer.flatMap(ccf_iterate_reducer)\n",
        "        dedup_output = ccf_iterate_output.distinct()\n",
        "\n",
        "        # Mise Ã  jour de la RDD des composantes\n",
        "        node_component_rdd = dedup_output\n",
        "\n",
        "        # Action obligatoire pour forcer lâ€™Ã©valuation du compteur (Spark est paresseux)\n",
        "        node_component_rdd.count()\n",
        "\n",
        "        # Si aucune propagation nâ€™a eu lieu, on considÃ¨re quâ€™on a convergÃ©\n",
        "        if new_pairs_counter.value == 0:\n",
        "            print(f\"âœ… Convergence atteinte en {iteration} itÃ©rations.\")\n",
        "            break\n",
        "\n",
        "    exec_time = time.time() - start_time\n",
        "    return node_component_rdd, iteration, exec_time\n",
        "\n",
        "# Chargement des fichiers et exÃ©cution du CCF pour chacun ---\n",
        "schema = StructType([\n",
        "    StructField(\"source\", IntegerType(), True),\n",
        "    StructField(\"target\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Liste des fichiers Ã  analyser\n",
        "files = [\n",
        "    (\"G1_1k.csv\", \"G1\"),\n",
        "    (\"G2_5k.csv\", \"G2\"),\n",
        "    (\"G3_8k.csv\", \"G3\"),\n",
        "    (\"G4_10k.csv\", \"G4\")\n",
        "]\n",
        "\n",
        "# Liste pour stocker les rÃ©sultats\n",
        "results = []\n",
        "\n",
        "for filename, label in files:\n",
        "    filepath = f\"data/{filename}\"  # Etre sÃ»r que le fichier est bien dans le dossier \"data\"\n",
        "    print(f\"ğŸ“ Traitement de {label} ({filepath})...\")\n",
        "\n",
        "    try:\n",
        "        # Chargement du fichier CSV en DataFrame structurÃ©\n",
        "        df = spark.read.csv(filepath, header=True, schema=schema)\n",
        "\n",
        "        # Conversion en RDD de paires (source, target)\n",
        "        edges_rdd = df.rdd.map(lambda row: (row['source'], row['target']))\n",
        "\n",
        "        # Lancement de l'algorithme CCF\n",
        "        components, num_iters, exec_time = ccf_correct_implementation(edges_rdd, max_iters=20)\n",
        "\n",
        "        # ğŸ“ˆ Statistiques du graphe\n",
        "        nb_nodes = edges_rdd.flatMap(lambda edge: [edge[0], edge[1]]).distinct().count()\n",
        "        nb_edges = edges_rdd.count()\n",
        "\n",
        "        print(f\"ğŸ“Š DonnÃ©es du graphe: {nb_nodes} nÅ“uds, {nb_edges} arÃªtes\")\n",
        "        print(f\"ğŸ” ItÃ©rations : {num_iters}\")\n",
        "        print(f\"â±ï¸ Temps : {round(exec_time, 3)} secondes\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Ajout aux rÃ©sultats\n",
        "        results.append((label, nb_nodes, nb_edges, num_iters, round(exec_time, 3)))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erreur avec {label} : {e}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "# Affichage final dans un tableau Pandas\n",
        "result_rdd_df = pd.DataFrame(\n",
        "    results,\n",
        "    columns=[\"Graphe\", \"NÅ“uds\", \"ArÃªtes\", \"ItÃ©rations\", \"Temps (s)\"]\n",
        ")\n",
        "\n",
        "print(\"âœ… RÃ©sumÃ© des performances (RDD) :\")\n",
        "print(result_rdd_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2 \tImplÃ©mentation CCF avec DataFrames _ Python**"
      ],
      "metadata": {
        "id": "nwAnmF7B4PFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, least\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType\n",
        "\n",
        "# CrÃ©ation de la session Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CCF DataFrame Correct\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "def ccf_dataframe_implementation(edges_df, max_iters=20):\n",
        "    \"\"\"\n",
        "    ImplÃ©mentation de lâ€™algorithme Connected Component Finder (CCF)\n",
        "    en utilisant l'API DataFrame de PySpark.\n",
        "\n",
        "    Objectif : identifier les composantes connexes d'un graphe non orientÃ©\n",
        "    Ã  partir de ses arÃªtes (edges_df), en propageant les identifiants\n",
        "    de composantes les plus petits jusqu'Ã  convergence.\n",
        "    \"\"\"\n",
        "\n",
        "   # Ã‰tape 1 : Initialisation â€” chaque nÅ“ud reÃ§oit une Ã©tiquette Ã©gale Ã  son propre ID\n",
        "    nodes = edges_df.select(\"source\").union(edges_df.select(\"target\")) \\\n",
        "        .distinct() \\\n",
        "        .withColumnRenamed(\"source\", \"node\")\n",
        "\n",
        "    labels = nodes.withColumn(\"component_id\", col(\"node\"))\n",
        "\n",
        "    iteration = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    # On transforme le graphe en liste d'adjacence non orientÃ©e (chaque lien est doublÃ©)\n",
        "    adj_list = edges_df.select(\"source\", \"target\").union(\n",
        "        edges_df.select(col(\"target\").alias(\"source\"), col(\"source\").alias(\"target\"))\n",
        "    )\n",
        "\n",
        "    # Boucle principale : on propage les plus petits IDs de composantes\n",
        "    while iteration < max_iters:\n",
        "        iteration += 1\n",
        "        print(f\"ğŸ” DÃ©marrage de l'itÃ©ration {iteration}...\")\n",
        "\n",
        "        # On renomme la colonne pour Ã©viter les conflits lors des jointures\n",
        "        labels_renamed = labels.withColumnRenamed(\"component_id\", \"current_component_id\")\n",
        "\n",
        "        # 1ï¸ Chaque nÅ“ud va consulter les Ã©tiquettes de ses voisins\n",
        "        new_labels = adj_list.join(labels_renamed, adj_list.target == labels_renamed.node) \\\n",
        "            .select(\n",
        "                adj_list.source.alias(\"node\"),\n",
        "                labels_renamed.current_component_id.alias(\"neighbor_component_id\")\n",
        "            ) \\\n",
        "            .groupBy(\"node\") \\\n",
        "            .agg({\"neighbor_component_id\": \"min\"}) \\\n",
        "            .withColumnRenamed(\"min(neighbor_component_id)\", \"propagated_id\")\n",
        "\n",
        "        # 2ï¸ Jointure avec les Ã©tiquettes actuelles pour comparer les valeurs\n",
        "        current_and_new_labels = labels_renamed.join(new_labels, \"node\", \"left_outer\")\n",
        "\n",
        "        # 3ï¸ Mise Ã  jour : on garde lâ€™ID de composante le plus petit entre lâ€™actuel et le propagÃ©\n",
        "        updated_labels = current_and_new_labels \\\n",
        "            .withColumn(\n",
        "                \"new_label\",\n",
        "                least(col(\"current_component_id\"), col(\"propagated_id\"))\n",
        "            ) \\\n",
        "            .select(col(\"node\"), col(\"new_label\").alias(\"component_id\"))\n",
        "\n",
        "        # 4ï¸ VÃ©rification de convergence : on regarde sâ€™il y a encore des changements\n",
        "        changes = updated_labels \\\n",
        "            .join(labels.withColumnRenamed(\"component_id\", \"old_component_id\"), \"node\") \\\n",
        "            .filter(col(\"component_id\") != col(\"old_component_id\")) \\\n",
        "            .count()\n",
        "\n",
        "        # ğŸ” On prÃ©pare les labels pour la prochaine itÃ©ration\n",
        "        labels = updated_labels\n",
        "\n",
        "        # Si aucun changement nâ€™a eu lieu, câ€™est que lâ€™algorithme a convergÃ©\n",
        "        if changes == 0:\n",
        "            print(f\"âœ… Convergence atteinte en {iteration} itÃ©rations.\")\n",
        "            break\n",
        "\n",
        "    exec_time = time.time() - start_time\n",
        "    return labels, iteration, exec_time\n",
        "\n",
        "\n",
        "# DÃ©finition du schÃ©ma des fichiers CSV\n",
        "schema = StructType([\n",
        "    StructField(\"source\", IntegerType(), True),\n",
        "    StructField(\"target\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Liste des graphes Ã  traiter\n",
        "files = [\n",
        "    (\"G1_1k.csv\", \"G1\"),\n",
        "    (\"G2_5k.csv\", \"G2\"),\n",
        "    (\"G3_8k.csv\", \"G3\"),\n",
        "    (\"G4_10k.csv\", \"G4\")\n",
        "]\n",
        "\n",
        "# Liste pour stocker les rÃ©sultats de chaque traitement\n",
        "results = []\n",
        "\n",
        "# Traitement de chaque fichier un par un\n",
        "for filename, label in files:\n",
        "    filepath = f\"data/{filename}\"  # Les fichiers doivent Ãªtre placÃ©s dans un dossier 'data'\n",
        "    print(f\"ğŸ“ Traitement de {label} ({filepath})...\")\n",
        "\n",
        "    try:\n",
        "        # Chargement du fichier CSV sous forme de DataFrame\n",
        "        edges_df = spark.read.csv(filepath, header=True, schema=schema)\n",
        "\n",
        "        # ExÃ©cution de l'algorithme CCF\n",
        "        components_df, num_iters, exec_time = ccf_dataframe_implementation(edges_df, max_iters=20)\n",
        "\n",
        "        # Calcul du nombre de nÅ“uds et dâ€™arÃªtes dans le graphe\n",
        "        nb_nodes = edges_df.select(\"source\").union(edges_df.select(\"target\")).distinct().count()\n",
        "        nb_edges = edges_df.count()\n",
        "\n",
        "        print(f\"ğŸ“Š DonnÃ©es du graphe : {nb_nodes} nÅ“uds, {nb_edges} arÃªtes\")\n",
        "        print(f\"ğŸ” ItÃ©rations : {num_iters}\")\n",
        "        print(f\"â±ï¸ Temps d'exÃ©cution : {round(exec_time, 3)} secondes\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Stockage des rÃ©sultats\n",
        "        results.append((label, nb_nodes, nb_edges, num_iters, round(exec_time, 3)))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erreur lors du traitement de {label} : {e}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "# Affichage final des performances sous forme de tableau Pandas\n",
        "result_df = pd.DataFrame(\n",
        "    results,\n",
        "    columns=[\"Graphe\", \"NÅ“uds\", \"ArÃªtes\", \"ItÃ©rations\", \"Temps (s)\"]\n",
        ")\n",
        "\n",
        "print(\"âœ… RÃ©sumÃ© des performances (DataFrame) :\")\n",
        "print(result_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt0YUWXU6K9Y",
        "outputId": "8beceb0b-074f-4731-fb53-487b30171d99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“ Traitement de G1 (data/G1_1k.csv)...\n",
            "ğŸ” DÃ©marrage de l'itÃ©ration 1...\n",
            "ğŸ” DÃ©marrage de l'itÃ©ration 2...\n",
            "ğŸ” DÃ©marrage de l'itÃ©ration 3...\n",
            "ğŸ” DÃ©marrage de l'itÃ©ration 4...\n",
            "ğŸ” DÃ©marrage de l'itÃ©ration 5...\n",
            "ğŸ” DÃ©marrage de l'itÃ©ration 6...\n",
            "ğŸ” DÃ©marrage de l'itÃ©ration 7...\n",
            "ğŸ” DÃ©marrage de l'itÃ©ration 8...\n",
            "âœ… Convergence atteinte en 8 itÃ©rations.\n",
            "ğŸ“Š DonnÃ©es du graphe : 996 nÅ“uds, 3000 arÃªtes\n",
            "ğŸ” ItÃ©rations : 8\n",
            "â±ï¸ Temps d'exÃ©cution : 140.183 secondes\n",
            "----------------------------------------\n",
            "ğŸ“ Traitement de G2 (data/G2_5k.csv)...\n",
            "ğŸ” DÃ©marrage de l'itÃ©ration 1...\n",
            "ğŸ” DÃ©marrage de l'itÃ©ration 2...\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1 **Programmation en RDDs (PySpark) - Python**"
      ],
      "metadata": {
        "id": "uNiPDXMC376N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cAqROqr30gL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4159eff2-47f2-41be-ef1b-a44d451bd43a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìé Traitement de G1 (data/G1_1k.csv)...\n",
            "‚ùå Erreur avec G1 : [PATH_NOT_FOUND] Path does not exist: file:/content/data/G1_1k.csv.\n",
            "----------------------------------------\n",
            "üìé Traitement de G2 (data/G2_5k.csv)...\n",
            "‚ùå Erreur avec G2 : [PATH_NOT_FOUND] Path does not exist: file:/content/data/G2_5k.csv.\n",
            "----------------------------------------\n",
            "üìé Traitement de G3 (data/G3_8k.csv)...\n",
            "‚ùå Erreur avec G3 : [PATH_NOT_FOUND] Path does not exist: file:/content/data/G3_8k.csv.\n",
            "----------------------------------------\n",
            "üìé Traitement de G4 (data/G4_10k.csv)...\n",
            "‚ùå Erreur avec G4 : [PATH_NOT_FOUND] Path does not exist: file:/content/data/G4_10k.csv.\n",
            "----------------------------------------\n",
            "‚úÖ R√©sum√© des performances (RDD) :\n",
            "Empty DataFrame\n",
            "Columns: [Graphe, N≈ìuds, Ar√™tes, It√©rations, Temps (s)]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Initialize SparkSession and SparkContext\n",
        "spark = SparkSession.builder.appName(\"CCF Correct RDD\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# The accumulator for tracking new pairs, as described in the paper\n",
        "new_pairs_counter = sc.accumulator(0)\n",
        "\n",
        "def ccf_correct_implementation(edges_rdd, max_iters=20):\n",
        "    \"\"\"\n",
        "    Correctly implements the Connected Component Finder (CCF) algorithm\n",
        "    using PySpark RDDs based on the paper's methodology.\n",
        "    \"\"\"\n",
        "    global new_pairs_counter\n",
        "\n",
        "    # √âtape 1 : Initialisation des √©tiquettes (Component ID)\n",
        "    # L'ID de composant initial est le n≈ìud lui-m√™me\n",
        "    nodes = edges_rdd.flatMap(lambda edge: [edge[0], edge[1]]).distinct()\n",
        "    node_component_rdd = nodes.map(lambda node: (node, node))\n",
        "\n",
        "    # √âtape 2 : Cr√©er la liste des voisins (adjacency list)\n",
        "    # The paper's MapReduce logic for CCF-Iterate is to emit (u,v) and (v,u)\n",
        "    # for each edge, so all neighbors for a node are grouped together.\n",
        "    # This input is then used by the reducer to find the min_val.\n",
        "    neighbors_rdd = edges_rdd.flatMap(lambda x: [(x[0], x[1]), (x[1], x[0])])\n",
        "\n",
        "    iteration = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    # --- Boucle d'it√©rations CCF ---\n",
        "    while iteration < max_iters:\n",
        "        iteration += 1\n",
        "        print(f\"üîÅ D√©marrage de l'it√©ration {iteration}...\")\n",
        "\n",
        "        # R√©initialiser le compteur pour la nouvelle it√©ration\n",
        "        new_pairs_counter.value = 0\n",
        "\n",
        "        # Join the current component labels with the neighbors list to\n",
        "        # find the label of each neighbor.\n",
        "        joined_rdd = node_component_rdd.join(neighbors_rdd).map(lambda x: (x[1][1], x[1][0]))\n",
        "\n",
        "        # Group all labels associated with each node\n",
        "        input_for_reducer = joined_rdd.union(node_component_rdd).groupByKey()\n",
        "\n",
        "        # --- CCF-Iterate (Reducer Phase) ---\n",
        "        def ccf_iterate_reducer(key_values):\n",
        "            key, values_iter = key_values\n",
        "            values = list(values_iter)\n",
        "            min_val = min(values)\n",
        "\n",
        "            # The paper's reducer logic (Figure 2)\n",
        "            if min_val < key:\n",
        "                # Emit the (key, min_val) pair\n",
        "                yield (key, min_val)\n",
        "                # Emit new pairs for other values and increment the counter\n",
        "                for val in values:\n",
        "                    if val != min_val:\n",
        "                        new_pairs_counter.add(1)\n",
        "                        yield (val, min_val)\n",
        "            else:\n",
        "                # If the current key is the smallest or equal, just return its current label\n",
        "                yield (key, key)\n",
        "\n",
        "        ccf_iterate_output = input_for_reducer.flatMap(ccf_iterate_reducer)\n",
        "\n",
        "        # --- CCF-Dedup ---\n",
        "        # Deduplicate the output of CCF-Iterate\n",
        "        dedup_output = ccf_iterate_output.distinct()\n",
        "\n",
        "        # Update the RDD for the next iteration\n",
        "        node_component_rdd = dedup_output\n",
        "\n",
        "        # Trigger an action to force the loop to evaluate the new_pairs_counter\n",
        "        num_changes = node_component_rdd.count()\n",
        "\n",
        "        # Check for convergence based on the accumulator\n",
        "        if new_pairs_counter.value == 0:\n",
        "            print(f\"‚úÖ Convergence atteinte en {iteration} it√©rations.\")\n",
        "            break\n",
        "\n",
        "    exec_time = time.time() - start_time\n",
        "\n",
        "    return node_component_rdd, iteration, exec_time\n",
        "\n",
        "# --- Section de chargement et d'ex√©cution pour chaque fichier de graphe ---\n",
        "schema = StructType([\n",
        "    StructField(\"source\", IntegerType(), True),\n",
        "    StructField(\"target\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# The list of files to process\n",
        "files = [\n",
        "    (\"G1_1k.csv\", \"G1\"),\n",
        "    (\"G2_5k.csv\", \"G2\"),\n",
        "    (\"G3_8k.csv\", \"G3\"),\n",
        "    (\"G4_10k.csv\", \"G4\")\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "for filename, label in files:\n",
        "    filepath = f\"data/{filename}\" # Assuming the 'data' folder contains the files\n",
        "    print(f\"üìé Traitement de {label} ({filepath})...\")\n",
        "\n",
        "    try:\n",
        "        df = spark.read.csv(filepath, header=True, schema=schema)\n",
        "        edges_rdd = df.rdd.map(lambda row: (row['source'], row['target']))\n",
        "\n",
        "        components, num_iters, exec_time = ccf_correct_implementation(edges_rdd, max_iters=20)\n",
        "\n",
        "        nb_nodes = edges_rdd.flatMap(lambda edge: [edge[0], edge[1]]).distinct().count()\n",
        "        nb_edges = edges_rdd.count()\n",
        "\n",
        "        print(f\"üìä Donn√©es du graphe: {nb_nodes} n≈ìuds, {nb_edges} ar√™tes\")\n",
        "        print(f\"üîÅ It√©rations : {num_iters}\")\n",
        "        print(f\"‚è±Ô∏è Temps : {round(exec_time, 3)} secondes\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        results.append((label, nb_nodes, nb_edges, num_iters, round(exec_time, 3)))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur avec {label} : {e}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "# Afficher les r√©sultats dans un tableau\n",
        "result_rdd_df = pd.DataFrame(\n",
        "    results,\n",
        "    columns=[\"Graphe\", \"N≈ìuds\", \"Ar√™tes\", \"It√©rations\", \"Temps (s)\"]\n",
        ")\n",
        "\n",
        "print(\"‚úÖ R√©sum√© des performances (RDD) :\")\n",
        "print(result_rdd_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2 \tImpl√©mentation CCF avec DataFrames _ Python**"
      ],
      "metadata": {
        "id": "nwAnmF7B4PFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, least\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType\n",
        "\n",
        "# Initialisation de SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CCF DataFrame Correct\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "def ccf_dataframe_implementation(edges_df, max_iters=20):\n",
        "    \"\"\"\n",
        "    Impl√©mente l'algorithme Connected Component Finder (CCF)\n",
        "    en utilisant les DataFrames PySpark.\n",
        "    \"\"\"\n",
        "\n",
        "    # √âtape 1 : Initialisation des √©tiquettes\n",
        "    nodes = edges_df.select(\"source\").union(edges_df.select(\"target\")) \\\n",
        "        .distinct() \\\n",
        "        .withColumnRenamed(\"source\", \"node\")\n",
        "\n",
        "    labels = nodes.withColumn(\"component_id\", col(\"node\"))\n",
        "\n",
        "    iteration = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Cr√©er une liste d'adjacence bidirectionnelle\n",
        "    adj_list = edges_df.select(\"source\", \"target\").union(edges_df.select(col(\"target\").alias(\"source\"), col(\"source\").alias(\"target\")))\n",
        "\n",
        "    while iteration < max_iters:\n",
        "        iteration += 1\n",
        "        print(f\"üîÅ D√©marrage de l'it√©ration {iteration}...\")\n",
        "\n",
        "        # Propagation de l'√©tiquette minimale\n",
        "        # Renommer la colonne 'component_id' de 'labels' pour l'utiliser sans ambigu√Øt√©.\n",
        "        labels_renamed = labels.withColumnRenamed(\"component_id\", \"current_component_id\")\n",
        "\n",
        "        # 1. Joindre les √©tiquettes actuelles avec la liste d'adjacence\n",
        "        new_labels = adj_list.join(labels_renamed, adj_list.target == labels_renamed.node) \\\n",
        "            .select(adj_list.source.alias(\"node\"), labels_renamed.current_component_id.alias(\"neighbor_component_id\")) \\\n",
        "            .groupBy(\"node\") \\\n",
        "            .agg({\"neighbor_component_id\": \"min\"}) \\\n",
        "            .withColumnRenamed(\"min(neighbor_component_id)\", \"propagated_id\")\n",
        "\n",
        "        # 2. Joindre les √©tiquettes actuelles (renomm√©es) avec les nouvelles √©tiquettes propag√©es.\n",
        "        current_and_new_labels = labels_renamed.join(new_labels, \"node\", \"left_outer\")\n",
        "\n",
        "        # 3. Mettre √† jour l'√©tiquette si l'√©tiquette propag√©e est plus petite.\n",
        "        updated_labels = current_and_new_labels \\\n",
        "            .withColumn(\n",
        "                \"new_label\",\n",
        "                least(col(\"current_component_id\"), col(\"propagated_id\"))\n",
        "            ) \\\n",
        "            .select(col(\"node\"), col(\"new_label\").alias(\"component_id\"))\n",
        "\n",
        "        # V√©rification de la convergence\n",
        "        # Joindre les √©tiquettes mises √† jour avec les anciennes\n",
        "        # Renommer la colonne 'component_id' de 'labels' pour √©viter l'ambigu√Øt√©.\n",
        "        changes = updated_labels.join(labels.withColumnRenamed(\"component_id\", \"old_component_id\"), \"node\") \\\n",
        "            .filter(col(\"component_id\") != col(\"old_component_id\")) \\\n",
        "            .count()\n",
        "\n",
        "        # Mettre √† jour les √©tiquettes pour la prochaine it√©ration\n",
        "        labels = updated_labels\n",
        "\n",
        "        if changes == 0:\n",
        "            print(f\"‚úÖ Convergence atteinte en {iteration} it√©rations.\")\n",
        "            break\n",
        "\n",
        "    exec_time = time.time() - start_time\n",
        "\n",
        "    return labels, iteration, exec_time\n",
        "\n",
        "# --- Section de chargement et d'ex√©cution pour chaque fichier de graphe ---\n",
        "schema = StructType([\n",
        "    StructField(\"source\", IntegerType(), True),\n",
        "    StructField(\"target\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "files = [\n",
        "    (\"G1_1k.csv\", \"G1\"),\n",
        "    (\"G2_5k.csv\", \"G2\"),\n",
        "    (\"G3_8k.csv\", \"G3\"),\n",
        "    (\"G4_10k.csv\", \"G4\")\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "for filename, label in files:\n",
        "    filepath = f\"data/{filename}\" # Assumant que le dossier 'data' contient les fichiers\n",
        "    print(f\"üìé Traitement de {label} ({filepath})...\")\n",
        "\n",
        "    try:\n",
        "        # Charger les donn√©es en tant que DataFrame\n",
        "        edges_df = spark.read.csv(filepath, header=True, schema=schema)\n",
        "\n",
        "        components_df, num_iters, exec_time = ccf_dataframe_implementation(edges_df, max_iters=20)\n",
        "\n",
        "        nb_nodes = edges_df.select(\"source\").union(edges_df.select(\"target\")).distinct().count()\n",
        "        nb_edges = edges_df.count()\n",
        "\n",
        "        print(f\"üìä Donn√©es du graphe: {nb_nodes} n≈ìuds, {nb_edges} ar√™tes\")\n",
        "        print(f\"üîÅ It√©rations : {num_iters}\")\n",
        "        print(f\"‚è±Ô∏è Temps : {round(exec_time, 3)} secondes\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        results.append((label, nb_nodes, nb_edges, num_iters, round(exec_time, 3)))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur avec {label} : {e}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "# Afficher les r√©sultats dans un tableau\n",
        "result_df = pd.DataFrame(\n",
        "    results,\n",
        "    columns=[\"Graphe\", \"N≈ìuds\", \"Ar√™tes\", \"It√©rations\", \"Temps (s)\"]\n",
        ")\n",
        "\n",
        "print(\"‚úÖ R√©sum√© des performances (DataFrame) :\")\n",
        "print(result_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt0YUWXU6K9Y",
        "outputId": "e4d36405-eb91-4be1-ae43-ad63ae85aa72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìé Traitement de G1 (data/G1_1k.csv)...\n",
            "üîÅ D√©marrage de l'it√©ration 1...\n",
            "üîÅ D√©marrage de l'it√©ration 2...\n",
            "üîÅ D√©marrage de l'it√©ration 3...\n",
            "üîÅ D√©marrage de l'it√©ration 4...\n",
            "üîÅ D√©marrage de l'it√©ration 5...\n",
            "üîÅ D√©marrage de l'it√©ration 6...\n",
            "üîÅ D√©marrage de l'it√©ration 7...\n",
            "üîÅ D√©marrage de l'it√©ration 8...\n",
            "‚úÖ Convergence atteinte en 8 it√©rations.\n",
            "üìä Donn√©es du graphe: 996 n≈ìuds, 3000 ar√™tes\n",
            "üîÅ It√©rations : 8\n",
            "‚è±Ô∏è Temps : 135.066 secondes\n",
            "----------------------------------------\n",
            "üìé Traitement de G2 (data/G2_5k.csv)...\n",
            "üîÅ D√©marrage de l'it√©ration 1...\n",
            "üîÅ D√©marrage de l'it√©ration 2...\n",
            "üîÅ D√©marrage de l'it√©ration 3...\n",
            "üîÅ D√©marrage de l'it√©ration 4...\n",
            "üîÅ D√©marrage de l'it√©ration 5...\n",
            "üîÅ D√©marrage de l'it√©ration 6...\n",
            "üîÅ D√©marrage de l'it√©ration 7...\n",
            "üîÅ D√©marrage de l'it√©ration 8...\n",
            "‚úÖ Convergence atteinte en 8 it√©rations.\n",
            "üìä Donn√©es du graphe: 4983 n≈ìuds, 15000 ar√™tes\n",
            "üîÅ It√©rations : 8\n",
            "‚è±Ô∏è Temps : 100.337 secondes\n",
            "----------------------------------------\n",
            "üìé Traitement de G3 (data/G3_8k.csv)...\n",
            "üîÅ D√©marrage de l'it√©ration 1...\n",
            "üîÅ D√©marrage de l'it√©ration 2...\n",
            "üîÅ D√©marrage de l'it√©ration 3...\n",
            "üîÅ D√©marrage de l'it√©ration 4...\n",
            "üîÅ D√©marrage de l'it√©ration 5...\n",
            "üîÅ D√©marrage de l'it√©ration 6...\n",
            "üîÅ D√©marrage de l'it√©ration 7...\n",
            "üîÅ D√©marrage de l'it√©ration 8...\n",
            "üîÅ D√©marrage de l'it√©ration 9...\n",
            "‚úÖ Convergence atteinte en 9 it√©rations.\n",
            "üìä Donn√©es du graphe: 7971 n≈ìuds, 24000 ar√™tes\n",
            "üîÅ It√©rations : 9\n",
            "‚è±Ô∏è Temps : 269.291 secondes\n",
            "----------------------------------------\n",
            "üìé Traitement de G4 (data/G4_10k.csv)...\n",
            "üîÅ D√©marrage de l'it√©ration 1...\n",
            "üîÅ D√©marrage de l'it√©ration 2...\n",
            "üîÅ D√©marrage de l'it√©ration 3...\n",
            "üîÅ D√©marrage de l'it√©ration 4...\n",
            "üîÅ D√©marrage de l'it√©ration 5...\n",
            "üîÅ D√©marrage de l'it√©ration 6...\n",
            "üîÅ D√©marrage de l'it√©ration 7...\n",
            "üîÅ D√©marrage de l'it√©ration 8...\n",
            "‚úÖ Convergence atteinte en 8 it√©rations.\n",
            "üìä Donn√©es du graphe: 9979 n≈ìuds, 30000 ar√™tes\n",
            "üîÅ It√©rations : 8\n",
            "‚è±Ô∏è Temps : 108.962 secondes\n",
            "----------------------------------------\n",
            "‚úÖ R√©sum√© des performances (DataFrame) :\n",
            "  Graphe  N≈ìuds  Ar√™tes  It√©rations  Temps (s)\n",
            "0     G1    996    3000           8    135.066\n",
            "1     G2   4983   15000           8    100.337\n",
            "2     G3   7971   24000           9    269.291\n",
            "3     G4   9979   30000           8    108.962\n"
          ]
        }
      ]
    }
  ]
}